{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurosymbolic Software Tutorial - ECG Dataset\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/kavigupta/neurosym-lib/blob/main/tutorial/ecg_exercise.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## Instruction\n",
    "- Navigating this notebook on Google Colab: There will be text blocks and code blocks throughout the notebook. The text blocks, such as this one, will contain instructions and questions for you to consider. The code blocks, such as the one below, will contain executible code. Sometimes you will have to modify the code blocks following the instructions in the text blocks. You can run the code block by either pressing control/cmd + enter or by clicking the arrow on left-hand side.\n",
    "- Saving Work: If you wish to save your work in this .ipynb, we recommend downloading the compressed repository from GitHub, unzipping it, uploading it to Google Drive, and opening this notebook from within Google Drive.\n",
    "\n",
    "## Notebook\n",
    "\n",
    "In this notebook, you will construct a DSL to analyze ECG data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import neurosym as ns\n",
    "from neurosym.examples import near\n",
    "\n",
    "pl = ns.import_pytorch_lightning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We then load and plot some bouncing ball trajectories. Note that these trajectories are represented as a list `[x, y, vx, vy]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset_npz(features_pth, label_pth):\n",
    "    assert os.path.exists(features_pth), f\"{features_pth} does not exist.\"\n",
    "    assert os.path.exists(label_pth), f\"{label_pth} does not exist.\"\n",
    "    X = np.load(features_pth)\n",
    "    y = np.load(label_pth)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def filter_multilabel(split):\n",
    "    x_fname = f\"ecg_exercise/x_{split}.npy\"\n",
    "    y_fname = f\"ecg_exercise/y_{split}.npy\"\n",
    "    X = np.load(x_fname)\n",
    "    y = np.load(y_fname)\n",
    "\n",
    "    mask = y.sum(-1) == 1\n",
    "\n",
    "    # filter\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    # normalize each column of X to [0, 1]\n",
    "    X = (X - X.min(0)) / (X.max(0) - X.min(0))\n",
    "\n",
    "    # save as filtered\n",
    "    np.save(x_fname.replace(f\"{split}\", f\"{split}_filtered\"), X.astype(np.float32))\n",
    "    np.save(y_fname.replace(f\"{split}\", f\"{split}_filtered\"), y.astype(np.float32))\n",
    "\n",
    "\n",
    "filter_multilabel(\"train\")\n",
    "filter_multilabel(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dataset_factory = lambda train_seed: ns.DatasetWrapper(\n",
    "    ns.DatasetFromNpy(\n",
    "        f\"ecg_exercise/x_train_filtered.npy\",\n",
    "        f\"ecg_exercise/y_train_filtered.npy\",\n",
    "        train_seed,\n",
    "    ),\n",
    "    ns.DatasetFromNpy(\n",
    "        f\"ecg_exercise/x_test_filtered.npy\",\n",
    "        f\"ecg_exercise/y_test_filtered.npy\",\n",
    "        None,\n",
    "    ),\n",
    "    batch_size=200,\n",
    ")\n",
    "datamodule = dataset_factory(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def plot_trajectory(trajectory, color):\n",
    "    # TODO: What is a good way to visualize the trajectory?\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plot_trajectory(datamodule.train.inputs[:], f\"C{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: DSL\n",
    "\n",
    "Fill in the `bounce_dsl` to parameterize the space of functions that could represent the trajectories of bouncing balls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.train.get_io_dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def subset_selector_all_feat(x, channel, typ):\n",
    "    x = x.reshape(-1, 12, 6, 2)\n",
    "    typ_idx = torch.full(\n",
    "        size=(x.shape[0],), fill_value=(0 if typ == \"interval\" else 1), device=x.device\n",
    "    )\n",
    "    channel_mask = channel(x.reshape(-1, 144))  # [B, 12]\n",
    "    masked_x = (x * channel_mask[..., None, None]).sum(1)\n",
    "    return masked_x[torch.arange(x.shape[0]), :, typ_idx]\n",
    "\n",
    "# def subset_selector_all_feat(x, channel, typ):\n",
    "#     x = x.reshape(-1, 12, 6, 2)\n",
    "#     typ_idx = torch.full(\n",
    "#         size=(x.shape[0],), fill_value=(0 if typ == \"interval\" else 1), device=x.device\n",
    "#     )\n",
    "#     channel_mask = channel(x.reshape(-1, 144))  # [B, 12]\n",
    "#     masked_x = (x * channel_mask[..., None, None]).sum(1)\n",
    "#     return masked_x[torch.arange(x.shape[0]), :, typ_idx]\n",
    "\n",
    "# def guard_callables(fn, **kwargs):\n",
    "#     is_callable = [callable(kwargs[k]) for k in kwargs]\n",
    "#     if any(is_callable):\n",
    "#         return lambda z: fn(\n",
    "#             **{\n",
    "#                 k: (kwargs[k](z) if is_callable[i] else kwargs[k])\n",
    "#                 for i, k in enumerate(kwargs)\n",
    "#             }\n",
    "#         )\n",
    "#     else:\n",
    "#         return fn(**kwargs)\n",
    "\n",
    "# def filter_constants(x):\n",
    "#     match x:\n",
    "#         case ns.ArrowType(a, b):\n",
    "#             return filter_constants(a) and filter_constants(b)\n",
    "#         case ns.AtomicType(a):\n",
    "#             return a not in [\"channel\", \"feature\"]\n",
    "#         case _:\n",
    "#             return True\n",
    "\n",
    "# def filter_same_type(x):\n",
    "#     raise NotImplementedError\n",
    "\n",
    "# def ecg_dsl():\n",
    "#     L = 144\n",
    "#     O = 2\n",
    "#     F = 6\n",
    "#     dslf = ns.DSLFactory(L=L, O=O, F=F, max_overall_depth=10)\n",
    "#     dslf.typedef(\"fInp\", \"{f, $L}\")\n",
    "#     dslf.typedef(\"fOut\", \"{f, $O}\")\n",
    "#     dslf.typedef(\"fFeat\", \"{f, $F}\")\n",
    "\n",
    "#     for i in range(12):\n",
    "#         dslf.concrete(\n",
    "#             f\"channel_{i}\",\n",
    "#             \"() -> () -> channel\",\n",
    "#             # onehot vector where the ith element is 1\n",
    "#             # lambda: lambda x: torch.full(\n",
    "#             #     tuple(x.shape[:-1] + (1,)), i, device=x.device\n",
    "#             # ),\n",
    "#             lambda: lambda x: torch.nn.functional.one_hot(\n",
    "#                 torch.full(tuple(x.shape[:-1]), i, device=x.device, dtype=torch.long),\n",
    "#                 num_classes=12,\n",
    "#             ),\n",
    "#         )\n",
    "\n",
    "#     dslf.concrete(\n",
    "#         \"select_interval\",\n",
    "#         \"(() -> channel) -> ($fInp) -> $fFeat\",\n",
    "#         lambda ch: lambda x: subset_selector_all_feat(x, ch, \"interval\"),\n",
    "#     )\n",
    "\n",
    "#     dslf.concrete(\n",
    "#         \"select_amplitude\",\n",
    "#         \"(() -> channel) -> ($fInp) -> $fFeat\",\n",
    "#         lambda ch: lambda x: subset_selector_all_feat(x, ch, \"amplitude\"),\n",
    "#     )\n",
    "\n",
    "#     dslf.filtered_type_variable(\"num\", lambda x: filter_constants(x))\n",
    "#     dslf.filtered_type_variable(\"num\", lambda x: filter_same_type(x))\n",
    "#     dslf.concrete(\n",
    "#         \"add\",\n",
    "#         \"(%num, %num) -> %num\",\n",
    "#         lambda x, y: guard_callables(fn=lambda x, y: x + y, x=x, y=y),\n",
    "#     )\n",
    "#     dslf.concrete(\n",
    "#         \"mul\",\n",
    "#         \"(%num, %num) -> %num\",\n",
    "#         lambda x, y: guard_callables(fn=lambda x, y: x * y, x=x, y=y),\n",
    "#     )\n",
    "\n",
    "#     dslf.parameterized(\n",
    "#         \"linear\",\n",
    "#         \"(($fInp) -> $fFeat) -> $fInp -> {f, 1}\",\n",
    "#         lambda f, lin: lambda x: lin(f(x)),\n",
    "#         dict(lin=lambda: nn.Linear(F, 1)),\n",
    "#     )\n",
    "\n",
    "#     dslf.parameterized(\n",
    "#         \"output\",\n",
    "#         \"(($fInp) -> $fFeat) -> $fInp -> $fOut\",\n",
    "#         lambda f, lin: lambda x: lin(f(x)),\n",
    "#         dict(lin=lambda: nn.Linear(F, O)),\n",
    "#     )\n",
    "\n",
    "#     # dslf.concrete(\"ite_ab\", \"(#a -> f, #a -> #b, #a -> #b) -> #a -> #b\", near.operations.ite_torch)\n",
    "#     # dslf.concrete(\"ite_aa\", \"(#a -> f, #a -> #a, #a -> #a) -> #a -> #a\", near.operations.ite_torch)\n",
    "#     # dslf.concrete(\n",
    "#     #     \"map\", \"(#a -> #b) -> [#a] -> [#b]\", lambda f: lambda x: near.operations.map_torch(f, x)\n",
    "#     # )\n",
    "#     dslf.prune_to(\"($fInp) -> $fOut\")\n",
    "#     return dslf.finalize()\n",
    "# dsl = ecg_dsl()\n",
    "\n",
    "\n",
    "from neurosym.dsl.dsl_factory import DSLFactory\n",
    "from neurosym.examples.near.operations.basic import ite_torch\n",
    "from neurosym.types.type import ArrowType, AtomicType\n",
    "\n",
    "\n",
    "def ecg_dsl(input_dim, output_dim, max_overall_depth=6):\n",
    "    \"\"\"Creates a domain-specific language (DSL) for neural symbolic computation.\n",
    "\n",
    "    This function sets up a DSL with basic operations like addition, multiplication,\n",
    "    and folds, as well as neural network components like linear layers.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): The dimensionality of the input features.\n",
    "        output_dim (int): The dimensionality of the output features.\n",
    "\n",
    "    Returns:\n",
    "        DSLFactory: An instance of `DSLFactory` with the defined operations and types.\n",
    "    \"\"\"\n",
    "    feature_dim = 6\n",
    "    dslf = DSLFactory(\n",
    "        I=input_dim, O=output_dim, F=feature_dim, max_overall_depth=max_overall_depth\n",
    "    )\n",
    "    dslf.typedef(\"fInp\", \"{f, $I}\")\n",
    "    dslf.typedef(\"fOut\", \"{f, $O}\")\n",
    "    dslf.typedef(\"fFeat\", \"{f, $F}\")\n",
    "\n",
    "    # \"add(select_interval(channel_2), select_amplitude(channel_8))\n",
    "    # \"add(select_interval_channel_2, select_amplitude_channel_8)\n",
    "    # \"add(??, ??)\"\n",
    "\n",
    "    for i in range(12):\n",
    "        dslf.concrete(\n",
    "            f\"channel_{i}\",\n",
    "            \"() -> () -> channel\",\n",
    "            # onehot vector where the ith element is 1\n",
    "            # lambda: lambda x: torch.full(\n",
    "            #     tuple(x.shape[:-1] + (1,)), i, device=x.device\n",
    "            # ),\n",
    "            lambda: lambda x: torch.nn.functional.one_hot(\n",
    "                torch.full(tuple(x.shape[:-1]), i, device=x.device, dtype=torch.long),\n",
    "                num_classes=12,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # for i in range(6):\n",
    "    #     dslf.concrete(\n",
    "    #         f\"feature_{i}\",\n",
    "    #         \"() -> () -> feature\",\n",
    "    #         lambda: lambda x: torch.full(\n",
    "    #             tuple(x.shape[:-1] + (1,)), i, device=x.device\n",
    "    #         ),\n",
    "    #     )\n",
    "\n",
    "    dslf.concrete(\n",
    "        \"select_interval\",\n",
    "        \"(() -> channel) -> ($fInp) -> $fFeat\",\n",
    "        lambda ch: lambda x: subset_selector_all_feat(x, ch, \"interval\"),\n",
    "    )\n",
    "\n",
    "    dslf.concrete(\n",
    "        \"select_amplitude\",\n",
    "        \"(() -> channel) -> ($fInp) -> $fFeat\",\n",
    "        lambda ch: lambda x: subset_selector_all_feat(x, ch, \"amplitude\"),\n",
    "    )\n",
    "\n",
    "    def guard_callables(fn, **kwargs):\n",
    "        is_callable = [callable(kwargs[k]) for k in kwargs]\n",
    "        if any(is_callable):\n",
    "            return lambda z: fn(\n",
    "                **{\n",
    "                    k: (kwargs[k](z) if is_callable[i] else kwargs[k])\n",
    "                    for i, k in enumerate(kwargs)\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            return fn(**kwargs)\n",
    "\n",
    "    def filter_constants(x):\n",
    "        match x:\n",
    "            case ArrowType(a, b):\n",
    "                return filter_constants(a) and filter_constants(b)\n",
    "            case AtomicType(a):\n",
    "                return a not in [\"channel\", \"feature\"]\n",
    "            case _:\n",
    "                return True\n",
    "\n",
    "    # def filter_same_type(x):\n",
    "    #     raise NotImplementedError\n",
    "\n",
    "    dslf.filtered_type_variable(\"num\", lambda x: filter_constants(x))\n",
    "    # dslf.filtered_type_variable(\"num\", lambda x: filter_same_type(x))\n",
    "    dslf.concrete(\n",
    "        \"add\",\n",
    "        \"(%num, %num) -> %num\",\n",
    "        lambda x, y: guard_callables(fn=lambda x, y: x + y, x=x, y=y),\n",
    "    )\n",
    "    dslf.concrete(\n",
    "        \"mul\",\n",
    "        \"(%num, %num) -> %num\",\n",
    "        lambda x, y: guard_callables(fn=lambda x, y: x * y, x=x, y=y),\n",
    "    )\n",
    "    # dslf.concrete(\n",
    "    #     \"sub\",\n",
    "    #     \"(%num, %num) -> %num\",\n",
    "    #     lambda x, y: guard_callables(fn=lambda x, y: x - y, x=x, y=y),\n",
    "    # )\n",
    "\n",
    "    # dslf.parameterized(\"linear_bool\", \"() -> $fFeat -> $fFeat\", lambda lin: lin, dict(lin=lambda: nn.Linear(input_dim, 1)))\n",
    "    dslf.parameterized(\n",
    "        \"linear\",\n",
    "        \"(($fInp) -> $fFeat) -> $fInp -> {f, 1}\",\n",
    "        lambda f, lin: lambda x: lin(f(x)),\n",
    "        dict(lin=lambda: nn.Linear(feature_dim, 1)),\n",
    "    )\n",
    "\n",
    "    dslf.parameterized(\n",
    "        \"output\",\n",
    "        \"(($fInp) -> $fFeat) -> $fInp -> $fOut\",\n",
    "        lambda f, lin: lambda x: lin(f(x)),\n",
    "        dict(lin=lambda: nn.Linear(feature_dim, output_dim)),\n",
    "    )\n",
    "\n",
    "    # dslf.concrete(\"iteA\", \"(#a -> $fFeat, #a -> #a, #a -> #a) -> #a -> #a\", lambda cond, fx, fy: ite_torch(cond, fx, fy))\n",
    "    dslf.concrete(\n",
    "        \"ite\",\n",
    "        \"(#a -> {f, 1}, #a -> #b, #a -> #b) -> #a -> #b\",\n",
    "        lambda cond, fx, fy: ite_torch(cond, fx, fy),\n",
    "        # lambda cond, fx, fy: guard_callables(fn=partial(ite_torch, condition=cond), if_true=fx, if_else=fy),\n",
    "    )\n",
    "    # dslf.concrete(\"map\", \"(#a -> #b) -> [#a] -> [#b]\", lambda f: lambda x: map_torch(f, x))\n",
    "    # dslf.concrete(\"compose\", \"(#a -> #b, #b -> #c) -> #a -> #c\", lambda f, g: lambda x: g(f(x)))\n",
    "    # dslf.concrete(\"fold\", \"((#a, #a) -> #a) -> [#a] -> #a\", lambda f: lambda x: fold_torch(f, x))\n",
    "\n",
    "    dslf.prune_to(\"($fInp) -> $fOut\")\n",
    "    return dslf.finalize(), dslf.t\n",
    "\n",
    "input_dim, output_dim = 144, 9\n",
    "\n",
    "dsl, dsl_type_env = ecg_dsl(input_dim=input_dim, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSL Printout\n",
    "\n",
    "See your DSL printed below, and ensure it is what you would expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      channel_0 :: () -> () -> channel\n",
      "      channel_1 :: () -> () -> channel\n",
      "      channel_2 :: () -> () -> channel\n",
      "      channel_3 :: () -> () -> channel\n",
      "      channel_4 :: () -> () -> channel\n",
      "      channel_5 :: () -> () -> channel\n",
      "      channel_6 :: () -> () -> channel\n",
      "      channel_7 :: () -> () -> channel\n",
      "      channel_8 :: () -> () -> channel\n",
      "      channel_9 :: () -> () -> channel\n",
      "     channel_10 :: () -> () -> channel\n",
      "     channel_11 :: () -> () -> channel\n",
      "select_interval :: (() -> channel) -> {f, 144} -> {f, 6}\n",
      "select_amplitude :: (() -> channel) -> {f, 144} -> {f, 6}\n",
      "            add :: (%num, %num) -> %num\n",
      "            mul :: (%num, %num) -> %num\n",
      "            ite :: (#a -> {f, 1}, #a -> #b, #a -> #b) -> #a -> #b\n",
      "    linear[lin] :: ({f, 144} -> {f, 6}) -> {f, 144} -> {f, 1}\n",
      "    output[lin] :: ({f, 144} -> {f, 6}) -> {f, 144} -> {f, 9}\n"
     ]
    }
   ],
   "source": [
    "print(dsl.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Neural DSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "t = ns.TypeDefiner(L=input_dim, O=output_dim)\n",
    "t.typedef(\"fL\", \"{f, $L}\")\n",
    "neural_dsl = near.NeuralDSL.from_dsl(\n",
    "    dsl=dsl,\n",
    "    modules={\n",
    "        **near.create_modules(\n",
    "            \"mlp\",\n",
    "            [\n",
    "                dsl_type_env(\"($fInp) -> $fInp\"),\n",
    "                dsl_type_env(\"($fInp) -> $fOut\"),\n",
    "                dsl_type_env(\"($fInp) -> $fFeat\"),\n",
    "                dsl_type_env(\"($fInp) -> {f, 1}\"),\n",
    "                #  t(\"([$fI]) -> [$fI]\"), t(\"([$fI]) -> [$fO]\")\n",
    "            ],\n",
    "            near.mlp_factory(hidden_size=10),\n",
    "        ),\n",
    "        **near.create_modules(\n",
    "            \"constant_int\",\n",
    "            [dsl_type_env(\"() -> channel\")],\n",
    "            near.selector_factory(input_dim=input_dim),\n",
    "            known_atom_shapes=dict(channel=(12,), feature=(6,)),\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "logging.getLogger(\"pytorch_lightning.utilities.rank_zero\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").setLevel(logging.WARNING)\n",
    "\n",
    "def cross_entropy_callback(predicitons, targets):\n",
    "    return torch.nn.functional.cross_entropy(predicitons, targets)\n",
    "\n",
    "trainer_cfg = near.NEARTrainerConfig(\n",
    "    lr=5e-3,\n",
    "    max_seq_len=300,\n",
    "    n_epochs=100,\n",
    "    num_labels=output_dim,\n",
    "    train_steps=len(datamodule.train),\n",
    "    loss_callback=cross_entropy_callback,\n",
    "    scheduler=\"cosine\",\n",
    "    optimizer=torch.optim.Adam,\n",
    ")\n",
    "\n",
    "validation_cost = near.ValidationCost(\n",
    "    neural_dsl=neural_dsl,\n",
    "    trainer_cfg=trainer_cfg,\n",
    "    datamodule=datamodule,\n",
    "    accelerator=\"cuda\",\n",
    "    devices=1,\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5)\n",
    "    ],\n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False,\n",
    "    progress_by_epoch=True,\n",
    ")\n",
    "\n",
    "g = near.near_graph(\n",
    "    neural_dsl,\n",
    "    ns.parse_type(\n",
    "        s=\"({f, $L}) -> {f, $O}\", env=ns.TypeDefiner(L=input_dim, O=output_dim)\n",
    "    ),\n",
    "    is_goal=neural_dsl.program_has_no_holes,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ??::<{f, 144} -> {f, 9}>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                          | 0/100 [00:00<?, ?it/s]/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:44: attribute 'config' removed from hparams because it cannot be pickled\n",
      "Training:  73%|████████████▍    | 73/100 [00:09<00:03,  7.95it/s, train_loss=1.97, val_loss=1.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (add ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (mul ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (ite ??::<{f, 144} -> {f, 1}> ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▋                 | 4/100 [00:00<00:14,  6.76it/s, train_loss=2.24, val_loss=2.26]/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:44: attribute 'config' removed from hparams because it cannot be pickled\n",
      "Training:   5%|▉                 | 5/100 [00:00<00:13,  6.89it/s, train_loss=2.22, val_loss=2.25]/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:44: attribute 'config' removed from hparams because it cannot be pickled\n",
      "Training:  45%|███████▋         | 45/100 [00:07<00:09,  6.05it/s, train_loss=1.86, val_loss=1.92]\n",
      "Training:  46%|███████▊         | 46/100 [00:07<00:09,  5.70it/s, train_loss=1.98, val_loss=1.91]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (output ??::<{f, 144} -> {f, 6}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|█▍                | 8/100 [00:01<00:11,  8.31it/s, train_loss=2.25, val_loss=2.21]\n",
      "Training:   8%|█▌                 | 8/100 [00:01<00:11,  8.31it/s, train_loss=2.23, val_loss=2.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (__neural_dsl_internal_mlp_2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|███████████▋     | 69/100 [00:11<00:04,  6.23it/s, train_loss=1.89, val_loss=1.88]\n",
      "Training:  23%|███▉             | 23/100 [00:02<00:08,  8.67it/s, train_loss=2.05, val_loss=2.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (mul (add ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>) ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████████▉     | 70/100 [00:08<00:03,  8.21it/s, train_loss=1.92, val_loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (mul (mul ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>) ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|███████████████▊  | 88/100 [00:10<00:01,  8.54it/s, train_loss=1.89, val_loss=1.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (mul (ite ??::<{f, 144} -> {f, 1}> ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>) ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████████▎       | 55/100 [00:08<00:07,  6.40it/s, train_loss=1.94, val_loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (mul (output ??::<{f, 144} -> {f, 6}>) ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|█████▊           | 34/100 [00:04<00:09,  7.04it/s, train_loss=1.92, val_loss=1.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (mul (__neural_dsl_internal_mlp_2) ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|██████▎          | 37/100 [00:06<00:10,  5.95it/s, train_loss=1.97, val_loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (add (add ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>) ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▉                 | 5/100 [00:00<00:13,  6.84it/s, train_loss=2.37, val_loss=2.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|███████▍          | 41/100 [00:06<00:07,  7.50it/s, train_loss=1.93, val_loss=1.9]Traceback (most recent call last):\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2102979/156620958.py\", line 7, in <module>\n",
      "    node = next(iterator)\n",
      "  File \"/home/asehgal/neurosym-lib/neurosym/search/bounded_astar_async.py\", line 80, in bounded_astar_async\n",
      "    fringe_var = fringe.getitem()\n",
      "  File \"/home/asehgal/neurosym-lib/neurosym/search/bounded_astar_async.py\", line 39, in getitem\n",
      "    self.syncronize()\n",
      "  File \"/home/asehgal/neurosym-lib/neurosym/search/bounded_astar_async.py\", line -1, in syncronize\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n",
      "Training:  42%|███████▏         | 42/100 [00:06<00:07,  7.58it/s, train_loss=1.91, val_loss=1.91]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|██████▊          | 40/100 [00:05<00:07,  7.72it/s, train_loss=1.92, val_loss=1.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (add (mul ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>) ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|███████▍         | 44/100 [00:06<00:08,  6.74it/s, train_loss=1.88, val_loss=1.94]\n",
      "Training:   9%|█▌                | 9/100 [00:01<00:13,  6.78it/s, train_loss=2.23, val_loss=2.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (add (ite ??::<{f, 144} -> {f, 1}> ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>) ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|███              | 18/100 [00:02<00:11,  7.14it/s, train_loss=2.07, val_loss=2.09]/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "/home/asehgal/env/miniconda3/envs/neurosym-lib/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "Training:  18%|███              | 18/100 [00:02<00:12,  6.79it/s, train_loss=2.07, val_loss=2.09]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (add (output ??::<{f, 144} -> {f, 6}>) ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (add (__neural_dsl_internal_mlp_2) ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (ite (add ??::<{f, 144} -> {f, 1}> ??::<{f, 144} -> {f, 1}>) ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|██████▎          | 37/100 [00:06<00:11,  5.59it/s, train_loss=1.96, val_loss=2.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (ite (mul ??::<{f, 144} -> {f, 1}> ??::<{f, 144} -> {f, 1}>) ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████████▌        | 50/100 [00:07<00:07,  6.47it/s, train_loss=1.95, val_loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (ite (ite ??::<{f, 144} -> {f, 1}> ??::<{f, 144} -> {f, 1}> ??::<{f, 144} -> {f, 1}>) ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|████████████▉    | 76/100 [00:11<00:03,  6.35it/s, train_loss=1.86, val_loss=1.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (ite (linear ??::<{f, 144} -> {f, 6}>) ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|█████▊           | 34/100 [00:06<00:12,  5.17it/s, train_loss=1.99, val_loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (ite (__neural_dsl_internal_mlp_4) ??::<{f, 144} -> {f, 9}> ??::<{f, 144} -> {f, 9}>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███████▊            | 39/100 [00:06<00:09,  6.16it/s, train_loss=1.98, val_loss=2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (output (select_interval ??::<() -> channel>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|█▌                | 9/100 [00:02<00:15,  5.80it/s, train_loss=2.23, val_loss=2.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (output (select_amplitude ??::<() -> channel>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████████▉        | 55/100 [00:08<00:07,  6.19it/s, train_loss=1.92, val_loss=1.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (output (mul ??::<{f, 144} -> {f, 6}> ??::<{f, 144} -> {f, 6}>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████| 100/100 [00:17<00:00,  5.71it/s, train_loss=2.06, val_loss=2.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (output (ite ??::<{f, 144} -> {f, 1}> ??::<{f, 144} -> {f, 6}> ??::<{f, 144} -> {f, 6}>))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|██████▎          | 37/100 [00:05<00:09,  6.67it/s, train_loss=1.97, val_loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (output (__neural_dsl_internal_mlp_3))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████| 100/100 [00:17<00:00,  5.56it/s, train_loss=2.03, val_loss=1.99]\n",
      "Training:  55%|█████████▎       | 55/100 [00:07<00:06,  7.29it/s, train_loss=1.94, val_loss=1.98]\n",
      "Training:  72%|████████████▏    | 72/100 [00:11<00:04,  6.00it/s, train_loss=1.86, val_loss=1.83]\n"
     ]
    }
   ],
   "source": [
    "# iterator = ns.search.bounded_astar(g, validation_cost, max_depth=16)\n",
    "iterator = ns.search.bounded_astar_async(g, validation_cost, max_depth=1, max_workers=3)\n",
    "best_program_nodes = []\n",
    "# Let's collect the top four programs\n",
    "while len(best_program_nodes) <= 3:\n",
    "    try:\n",
    "        node = next(iterator)\n",
    "        cost = validation_cost(node)\n",
    "        best_program_nodes.append((node, cost))\n",
    "        print(\"Got another program\")\n",
    "    except StopIteration:\n",
    "        print(\"No more programs found.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 Programs\n",
    "\n",
    "The code below assumes you found some top 3 programs and stored them in the best_program_nodes variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "best_program_nodes = sorted(best_program_nodes, key=lambda x: x[1])\n",
    "for i, (node, cost) in enumerate(best_program_nodes):\n",
    "    print(\n",
    "        \"({i}) Cost: {cost:.4f}, {program}\".format(\n",
    "            i=i, program=ns.render_s_expression(node.program), cost=cost\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is set up to further fine tune the program, test it, and return a set of values produced by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def testProgram(best_program_node):\n",
    "    module = near.TorchProgramModule(\n",
    "        dsl=neural_dsl, program=best_program_node[0].program\n",
    "    )\n",
    "    pl_model = near.NEARTrainer(module, config=trainer_cfg)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=4000,\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"cpu\",\n",
    "        enable_checkpointing=False,\n",
    "        logger=False,\n",
    "        callbacks=[\n",
    "            pl.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5)\n",
    "        ],\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "\n",
    "    trainer.fit(pl_model, datamodule.train_dataloader(), datamodule.val_dataloader())\n",
    "    # T = 100\n",
    "    # path = np.zeros((T, 4))\n",
    "    # X = torch.tensor(\n",
    "    #     np.array([0.21413583, 4.4062634, 3.4344807, 0.12440437]), dtype=torch.float32\n",
    "    # )\n",
    "    # for t in range(T):\n",
    "    #     path[t, :] = X.detach().numpy()\n",
    "    #     Y = module(X.unsqueeze(0)).squeeze(0)\n",
    "    #     X = Y\n",
    "    # return path\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# # We generate trajectories for the top 2 programs.\n",
    "# trajectory = testProgram(best_program_nodes[0])\n",
    "# trajectoryb = testProgram(best_program_nodes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 8))\n",
    "\n",
    "# plot_trajectory(trajectory, \"C0\")\n",
    "# plot_trajectory(trajectoryb, \"C1\")\n",
    "# plot_trajectory(datamodule.train.inputs[0], \"black\")\n",
    "\n",
    "# plt.title(\"Bouncing ball (ground truth in black)\")\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
