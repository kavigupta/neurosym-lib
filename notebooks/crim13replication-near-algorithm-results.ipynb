{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "\n",
    "import neurosym as ns\n",
    "from neurosym.examples import near\n",
    "\n",
    "LIST_LENGTH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the binary cross entropy loss with class weights for the Tiny CalMS21 dataset.\n",
    "    This is the same loss function used in the base NEAR implementation.\n",
    "\n",
    "    Args:\n",
    "        predictions (torch.Tensor): Model predictions with shape (B, T, O).\n",
    "        targets (torch.Tensor): Ground truth labels with shape (B, T, 1).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed binary cross-entropy loss.\n",
    "    \"\"\"\n",
    "    targets = targets.squeeze(-1)  # (B, T, 1) -> (B, T)\n",
    "    predictions = predictions.view(-1, predictions.shape[-1])\n",
    "    targets = targets.view(-1)\n",
    "    # pylint: disable=not-callable\n",
    "    targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=2)\n",
    "    # pylint: enable=not-callable\n",
    "    return torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "        predictions.float(),\n",
    "        targets_one_hot.float(),\n",
    "        weight=torch.tensor([1.0, 1.5], device=predictions.device),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_near_metrics(predictions, ground_truth):\n",
    "    weighted_avg_f1 = f1_score(predictions, ground_truth, average=\"weighted\")\n",
    "    unweighted_avg_f1 = f1_score(predictions, ground_truth, average=\"macro\")\n",
    "    all_f1 = f1_score(predictions, ground_truth, average=None)\n",
    "    hamming_accuracy = 1 - hamming_loss(ground_truth, predictions)\n",
    "    return dict(\n",
    "        f1_score=weighted_avg_f1,\n",
    "        unweighted_f1=unweighted_avg_f1,\n",
    "        all_f1s=all_f1,\n",
    "        hamming_accuracy=hamming_accuracy,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_crim13_experiment(\n",
    "    output_path: str = \"outputs/compression_near_logs/crim13_results.pkl\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the NEAR experiment on the CRIM13 dataset.\n",
    "\n",
    "    Args:\n",
    "        output_path (str): File path to save the resulting programs list as a pickle file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare data and DSL\n",
    "    datamodule = ns.datasets.crim13_data_example(train_seed=0, batch_size=1024)\n",
    "    _, output_dim = datamodule.train.get_io_dims()\n",
    "    original_dsl = near.simple_crim13_dsl(num_classes=output_dim, hidden_dim=16)\n",
    "\n",
    "    # Trainer configuration\n",
    "    trainer_cfg = near.NEARTrainerConfig(\n",
    "        n_epochs=15,\n",
    "        lr=1e-3,\n",
    "        loss_callback=bce_loss,\n",
    "    )\n",
    "\n",
    "    neural_dsl = near.NeuralDSL.from_dsl(\n",
    "        dsl=original_dsl,\n",
    "        neural_hole_filler=near.GenericMLPRNNNeuralHoleFiller(hidden_size=16),\n",
    "    )\n",
    "\n",
    "    cost = near.default_near_cost(\n",
    "        trainer_cfg=trainer_cfg,\n",
    "        datamodule=datamodule,\n",
    "        structural_cost_weight=0.05,\n",
    "    )\n",
    "\n",
    "    # Create the NEAR graph\n",
    "    g = near.near_graph(\n",
    "        neural_dsl,\n",
    "        neural_dsl.valid_root_types[0],\n",
    "        is_goal=lambda _: True,\n",
    "        cost=cost,\n",
    "    )\n",
    "\n",
    "    # Search for programs with bounded A*\n",
    "    iterator = ns.search.bounded_astar(g, max_depth=10)\n",
    "\n",
    "    programs_list = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Collect programs up to LIST_LENGTH\n",
    "    while True:\n",
    "        try:\n",
    "            program = next(iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        timer = time.time() - start_time\n",
    "        programs_list.append({\"program\": program, \"time\": timer})\n",
    "\n",
    "        if len(programs_list) >= LIST_LENGTH:\n",
    "            print(\"Programs list is too long\")\n",
    "            break\n",
    "\n",
    "    # Optional: Drop into an interactive shell (commented out for module usage)\n",
    "    # import IPython; IPython.embed()\n",
    "\n",
    "    # Evaluate each discovered program\n",
    "    for d in programs_list:\n",
    "        program = d[\"program\"]\n",
    "        initialized_program = neural_dsl.initialize(program)\n",
    "        _ = cost.validation_heuristic.with_n_epochs(40).compute_cost(\n",
    "            neural_dsl, initialized_program, cost.embedding\n",
    "        )\n",
    "\n",
    "        feature_data = datamodule.test.inputs\n",
    "        labels = datamodule.test.outputs.flatten()\n",
    "\n",
    "        module = ns.examples.near.TorchProgramModule(neural_dsl, initialized_program)\n",
    "        predictions = (\n",
    "            module(torch.tensor(feature_data), environment=())\n",
    "            .argmax(-1)\n",
    "            .numpy()\n",
    "            .flatten()\n",
    "        )\n",
    "        metrics = compute_near_metrics(predictions, labels)\n",
    "        d[\"report\"] = metrics\n",
    "\n",
    "    # Save the programs_list to a pickle file\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        pickle.dump(programs_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_crim13_experiment(\"../outputs/mice_results/crim13_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>time</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>unweighted_f1</th>\n",
       "      <th>hamming_accuracy</th>\n",
       "      <th>program</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>astar_neurosymlib</td>\n",
       "      <td>979.8875</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>(output (map (affine_distance)))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>astar_neurosymlib</td>\n",
       "      <td>979.8876</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.3919</td>\n",
       "      <td>0.642</td>\n",
       "      <td>(output (map (affine_position)))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>astar_neurosymlib</td>\n",
       "      <td>1114.0062</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.1259</td>\n",
       "      <td>0.1312</td>\n",
       "      <td>(output (map (add (affine_angle) (affine_dista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>astar_neurosymlib</td>\n",
       "      <td>1173.9176</td>\n",
       "      <td>0.8078</td>\n",
       "      <td>0.5069</td>\n",
       "      <td>0.8094</td>\n",
       "      <td>(output (map (add (affine_position) (affine_an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>astar_neurosymlib</td>\n",
       "      <td>1173.9176</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.8582</td>\n",
       "      <td>(output (map (add (affine_position) (affine_po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>astar_near</td>\n",
       "      <td>2275.7735</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>Start(MapPrefixes(Last5Avg(PositionSelect())))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>enumeration</td>\n",
       "      <td>271.8955</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>Start(Map(PositionSelect()))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iddfs_near</td>\n",
       "      <td>19691.4884</td>\n",
       "      <td>0.9439</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>Start(MapPrefixes(Last5Avg(AccelerationSelect(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              method        time f1_score unweighted_f1 hamming_accuracy  \\\n",
       "0  astar_neurosymlib    979.8875   0.9439        0.4719           0.8937   \n",
       "1  astar_neurosymlib    979.8876    0.585        0.3919            0.642   \n",
       "2  astar_neurosymlib   1114.0062   0.1903        0.1259           0.1312   \n",
       "3  astar_neurosymlib   1173.9176   0.8078        0.5069           0.8094   \n",
       "4  astar_neurosymlib   1173.9176   0.8704         0.564           0.8582   \n",
       "5         astar_near   2275.7735   0.9439        0.4719           0.8937   \n",
       "6        enumeration    271.8955   0.1076        0.4719           0.8937   \n",
       "7         iddfs_near  19691.4884   0.9439        0.4719           0.8937   \n",
       "\n",
       "                                             program  \n",
       "0                   (output (map (affine_distance)))  \n",
       "1                   (output (map (affine_position)))  \n",
       "2  (output (map (add (affine_angle) (affine_dista...  \n",
       "3  (output (map (add (affine_position) (affine_an...  \n",
       "4  (output (map (add (affine_position) (affine_po...  \n",
       "5     Start(MapPrefixes(Last5Avg(PositionSelect())))  \n",
       "6                       Start(Map(PositionSelect()))  \n",
       "7  Start(MapPrefixes(Last5Avg(AccelerationSelect(...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_obj(file_path: str) -> object:\n",
    "    \"\"\"Load an object from a JSON or pickle file.\"\"\"\n",
    "    path = Path(file_path)\n",
    "    if path.suffix == \".json\":\n",
    "        with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    elif path.suffix == \".pkl\":\n",
    "        with path.open(\"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {path.suffix}\")\n",
    "\n",
    "\n",
    "baseline_results = load_obj(\"../outputs/mice_results/baseline_results.json\")\n",
    "our_results = load_obj(\"../outputs/mice_results/crim13_results.pkl\")\n",
    "\n",
    "df1 = (\n",
    "    pd.DataFrame(our_results)\n",
    "    .assign(\n",
    "        program=lambda d: d[\"program\"].map(ns.render_s_expression),\n",
    "        method=\"astar_neurosymlib\",\n",
    "    )\n",
    "    .pipe(lambda d: d.join(d.pop(\"report\").apply(pd.Series)))\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "df2 = pd.DataFrame(baseline_results).T.reset_index(names=[\"method\"])\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True).dropna(axis=1)\n",
    "\n",
    "# Display desired columns\n",
    "final_df = df[[\"method\", \"time\", \"f1_score\", \"unweighted_f1\", \"hamming_accuracy\", \"program\"]]\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurosym-lib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
