{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALMS21 Investigation NEAR Results Analysis\n",
    "\n",
    "This notebook analyzes and compares results from:\n",
    "- Baseline NEAR experiments (if available)\n",
    "- Neurosym-lib NEAR experiments (from results pickle files)\n",
    "\n",
    "## Step 0: Data collection.\n",
    "\n",
    "Compute results for neurosym-lib experiments\n",
    "```bash\n",
    "uv run python notebooks/calms21_reproduction/benchmark_calms21.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asehgal/neurosym-lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asehgal/neurosym-lib/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n",
      "/home/asehgal/neurosym-lib/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /home/asehgal/neurosym-lib\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from neurosym.examples.near.metrics import compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best NEAR results\n",
    "near_results_pth = Path(\"outputs/calms21_results/near_outputs/\")\n",
    "\n",
    "baseline_results = {}\n",
    "for result_file in near_results_pth.rglob(\"test_results.json\"):\n",
    "    print(f\"Loading Baseline results from {result_file}\")\n",
    "    with open(result_file, \"r\") as f:\n",
    "        expt_report = json.load(f)\n",
    "        experiment_name = result_file.parent.name\n",
    "        y_true = np.array(expt_report[\"true_vals\"])\n",
    "        y_scores = np.array(expt_report[\"predicted_vals\"])\n",
    "        expt_report[\"report\"] = compute_metrics(y_scores, y_true)\n",
    "        baseline_results[experiment_name] = expt_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Neurosym-lib Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: outputs/calms21_results/reproduction.pkl not found\n"
     ]
    }
   ],
   "source": [
    "file_path = \"outputs/calms21_results/reproduction.pkl\"\n",
    "neurosym_results = {}\n",
    "if Path(file_path).exists():\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "        file_name = Path(file_path).stem\n",
    "        for i, result in enumerate(results):\n",
    "            key = f\"neurosym_{file_name}_{i:03d}\"\n",
    "            neurosym_results[key] = result\n",
    "    print(f\"  Loaded {len(results)} programs from {file_name}\")\n",
    "else:\n",
    "    print(f\"Warning: {file_path} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reported Program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reported_path = \"outputs/calms21_results/reported_program.pkl\"\n",
    "if Path(reported_path).exists():\n",
    "    with open(reported_path, \"rb\") as f:\n",
    "        reported_program_result = pickle.load(f)\n",
    "    print(f\"Loaded reported program results from {reported_path}\")\n",
    "else:\n",
    "    print(f\"Warning: {reported_path} not found\")\n",
    "    reported_program_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = {}\n",
    "all_results.update(baseline_results)\n",
    "all_results.update(neurosym_results)\n",
    "\n",
    "if reported_program_result is not None:\n",
    "    all_results[\"reported_program\"] = reported_program_result\n",
    "\n",
    "print(f\"Total experiments: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "table_data = []\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    # Extract metrics\n",
    "    report = result.get(\"report\", {})\n",
    "    \n",
    "    # Handle different report structures\n",
    "    if isinstance(report, dict):\n",
    "        if \"report\" in report:\n",
    "            macro_avg = report[\"report\"].get(\"macro avg\", {})\n",
    "            hamming_acc = report.get(\"hamming_accuracy\", 0.0)\n",
    "        elif \"macro avg\" in report:\n",
    "            macro_avg = report[\"macro avg\"]\n",
    "            hamming_acc = report.get(\"hamming_accuracy\", 0.0)\n",
    "        else:\n",
    "            # Need to compute metrics\n",
    "            if \"pred_vals\" in result and \"true_vals\" in result:\n",
    "                pred_vals = np.array(result[\"pred_vals\"])\n",
    "                true_vals = np.array(result[\"true_vals\"])\n",
    "                metrics = compute_metrics(pred_vals, true_vals)\n",
    "                macro_avg = metrics[\"report\"][\"macro avg\"]\n",
    "                hamming_acc = metrics[\"hamming_accuracy\"]\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    row = {\n",
    "        \"experiment\": name,\n",
    "        \"precision\": macro_avg.get(\"precision\", 0.0),\n",
    "        \"recall\": macro_avg.get(\"recall\", 0.0),\n",
    "        \"f1_score\": macro_avg.get(\"f1-score\", 0.0),\n",
    "        \"support\": macro_avg.get(\"support\", 0),\n",
    "        \"hamming_accuracy\": hamming_acc,\n",
    "        \"time\": result.get(\"time\", 0.0),\n",
    "    }\n",
    "    table_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(table_data)\n",
    "\n",
    "# Sort by hamming_accuracy descending\n",
    "df = df.sort_values(\"hamming_accuracy\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total experiments: {len(df)}\")\n",
    "print(f\"\\nBest F1-score: {df['f1_score'].max():.6f}\")\n",
    "print(f\"  Experiment: {df.loc[df['f1_score'].idxmax(), 'experiment']}\")\n",
    "print(f\"\\nBest Hamming accuracy: {df['hamming_accuracy'].max():.6f}\")\n",
    "print(f\"  Experiment: {df.loc[df['hamming_accuracy'].idxmax(), 'experiment']}\")\n",
    "print(f\"\\nAverage F1-score: {df['f1_score'].mean():.6f}\")\n",
    "print(f\"Average Hamming accuracy: {df['hamming_accuracy'].mean():.6f}\")\n",
    "print(f\"Average training time: {df['time'].mean():.2f} seconds\")\n",
    "\n",
    "# Baseline vs Neurosym comparison\n",
    "baseline_df = df[df['experiment'].str.contains('baseline|calms21_astar|calms21_iddfs|enumeration', case=False, na=False)]\n",
    "neurosym_df = df[df['experiment'].str.contains('neurosym|reproduction', case=False, na=False)]\n",
    "\n",
    "if not baseline_df.empty:\n",
    "    print(f\"\\nBaseline experiments: {len(baseline_df)}\")\n",
    "    print(f\"  Avg F1: {baseline_df['f1_score'].mean():.6f}\")\n",
    "    print(f\"  Avg Hamming Acc: {baseline_df['hamming_accuracy'].mean():.6f}\")\n",
    "    \n",
    "if not neurosym_df.empty:\n",
    "    print(f\"\\nNeurosym-lib experiments: {len(neurosym_df)}\")\n",
    "    print(f\"  Avg F1: {neurosym_df['f1_score'].mean():.6f}\")\n",
    "    print(f\"  Avg Hamming Acc: {neurosym_df['hamming_accuracy'].mean():.6f}\")\n",
    "\n",
    "# Reported program\n",
    "reported_df = df[df['experiment'] == 'reported_program']\n",
    "if not reported_df.empty:\n",
    "    print(\"\\nReported Program:\")\n",
    "    print(f\"  F1: {reported_df['f1_score'].values[0]:.6f}\")\n",
    "    print(f\"  Hamming Acc: {reported_df['hamming_accuracy'].values[0]:.6f}\")\n",
    "    print(f\"  Time: {reported_df['time'].values[0]:.2f}s\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "csv_path = \"outputs/calms21_results/comparison.csv\"\n",
    "Path(csv_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved CSV to: {csv_path}\")\n",
    "\n",
    "# Save Markdown\n",
    "md_path = \"outputs/calms21_results/comparison.md\"\n",
    "with open(md_path, \"w\") as f:\n",
    "    f.write(\"# CALMS21 Investigation NEAR Results Comparison\\n\\n\")\n",
    "    f.write(df.to_markdown(index=False))\n",
    "    f.write(\"\\n\")\n",
    "print(f\"Saved Markdown to: {md_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Top Programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 10 programs by F1 score\n",
    "print(\"\\nTop 10 Programs by F1-score:\")\n",
    "print(\"=\" * 80)\n",
    "top_10 = df.head(10)\n",
    "print(top_10.to_string(index=False))\n",
    "\n",
    "# Show program strings for top results if available\n",
    "print(\"\\nProgram Details:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, row in top_10.iterrows():\n",
    "    exp_name = row['experiment']\n",
    "    if exp_name in all_results:\n",
    "        result = all_results[exp_name]\n",
    "        program_str = result.get('program_str', str(result.get('program', 'N/A')))\n",
    "        print(f\"\\n{exp_name}:\")\n",
    "        print(f\"  Program: {program_str}\")\n",
    "        print(f\"  F1: {row['f1_score']:.6f}, Hamming Acc: {row['hamming_accuracy']:.6f}, Time: {row['time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
