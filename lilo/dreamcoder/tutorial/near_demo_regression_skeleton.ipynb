{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurosymbolic Software Tutorial - Regressions\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/kavigupta/neurosym-lib/blob/main/tutorial/near_demo_regression_skeleton.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## Instruction\n",
    "- Navigating this notebook on Google Colab: There will be text blocks and code blocks throughout the notebook. The text blocks, such as this one, will contain instructions and questions for you to consider. The code blocks, such as the one below, will contain executable code. Sometimes you will have to modify the code blocks following the instructions in the text blocks. You can run the code block by either pressing control/cmd + enter or by clicking the arrow on left-hand side as shown. `@TODO`\n",
    "- Saving Work: If you wish to save your work in this .ipynb, we recommend downloading the compressed repository from GitHub, unzipping it, uploading it to Google Drive, and opening this notebook from within Google Drive.\n",
    "\n",
    "\n",
    "\n",
    "## Outline\n",
    "- Part 1: Data Exploration\n",
    "    - We're going to define a function `datagen()` and plot trajectories generated with datagen.\n",
    "    - **Exercise**: Before reading through the code, look at the trajectory plot and hypothesize what the underlying function might be. Write down what mathematical operators (`sin`, `pow`, `exp`, etc.) would be useful to discover the underlying function.\n",
    "- Part 2: DSL Generation\n",
    "    - We're going to formalize our intuition by writing a DSL. Write code for the DSL.\n",
    "    - **Exercise**: Modify the DSL with the mathematical operators we wrote down earlier. \n",
    "- Part 3: Program Generation\n",
    "    - We're going to use Neural guided search (NEAR) to search for the best-fit program in the DSL.\n",
    "- Part 4: Program Inspection\n",
    "    - We will render the program found by NEAR and inspect it's performance. \n",
    "    - **Exercise**: Inspect the program found after search. Try different hyperparamters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Exploration\n",
    "- Cell 1: Define `datagen()` and save data.\n",
    "- Cell 2: Plot the saved data.\n",
    "- Cell 3: Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "IS_REGRESSION = True\n",
    "\n",
    "\n",
    "def datagen(B, T, *, seed, is_regression=False):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    # generates a numpy trajectory of shape\n",
    "    #   X = (B, T, d_inp)\n",
    "    # and a numpy target of shape\n",
    "    #   Y = (B, T, 1)\n",
    "    X = rng.rand(B, T, 2)\n",
    "\n",
    "    X[:, 0, :] = 0\n",
    "    for t in range(1, T):\n",
    "        step = rng.randn(B, 2) * 0.2\n",
    "        X[:, t, :] = X[:, t - 1, :] + step\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    # normalize X b/w -1 and 1\n",
    "    X = (X - np.min(X)) / (np.max(X) - np.min(X))\n",
    "    X = (X - 0.5) * 2\n",
    "\n",
    "    if is_regression:\n",
    "        # # y = distance from origin of X\n",
    "        # Y = np.zeros((B, T, 1))\n",
    "        # Y = np.linalg.norm(X, axis=2, keepdims=True)\n",
    "        # Y = Y.reshape(-1, T, 1)\n",
    "        # Y = Y.astype(np.float32)\n",
    "\n",
    "        Y = abs(X[:, :, 0]).reshape(B, T, 1)\n",
    "        # for all values of X > 0, y *= 10 else y *= -10\n",
    "        Y = np.where(X[:, :, 0:1] > 0, Y * 2, Y * 0.5)\n",
    "    else:\n",
    "        # y = quadrant of X\n",
    "        Y = np.zeros((B, T), dtype=int)\n",
    "        for i in range(B):\n",
    "            for j in range(T):\n",
    "                x, y = X[i, j, :]\n",
    "                if x > 0 and y > 0:\n",
    "                    Y[i, j] = 0\n",
    "                elif x < 0 and y > 0:\n",
    "                    Y[i, j] = 0\n",
    "                elif x < 0 and y < 0:\n",
    "                    Y[i, j] = 1\n",
    "                else:  # x > 0 and y < 0\n",
    "                    Y[i, j] = 1\n",
    "        Y = Y.reshape(B, T, 1)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X_train, y_train = datagen(1000, 10, is_regression=IS_REGRESSION, seed=1)\n",
    "X_test, y_test = datagen(50, 10, is_regression=IS_REGRESSION, seed=0)\n",
    "# save data\n",
    "os.makedirs(\"../data/regression_example/\", exist_ok=True)\n",
    "np.save(\"../data/regression_example/train_ex_data.npy\", X_train)\n",
    "np.save(\"../data/regression_example/train_ex_labels.npy\", y_train)\n",
    "np.save(\"../data/regression_example/test_ex_data.npy\", X_test)\n",
    "np.save(\"../data/regression_example/test_ex_labels.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "def plot_trajectory(X, Y, is_regression=False):\n",
    "    B, T, _ = X.shape\n",
    "\n",
    "    if is_regression:\n",
    "        cmap_name = \"jet\"\n",
    "        colorbar_label = \"Distance from origin\"\n",
    "        title = \"Trajectories and their distances from origin\"\n",
    "        norm = Normalize(vmin=Y.min(), vmax=Y.max())\n",
    "    else:\n",
    "        cmap_name = \"jet\"\n",
    "        colorbar_label = \"Quadrant\"\n",
    "        title = \"Trajectories and their quadrants\"\n",
    "        norm = Normalize(vmin=Y.min(), vmax=Y.max())\n",
    "\n",
    "    for b in range(B):\n",
    "        trajectory = X[b]\n",
    "        output = Y[b].squeeze()\n",
    "\n",
    "        plt.scatter(\n",
    "            trajectory[:, 0],\n",
    "            trajectory[:, 1],\n",
    "            c=output,\n",
    "            marker=\"o\",\n",
    "            cmap=cmap_name,\n",
    "            norm=norm,\n",
    "        )\n",
    "\n",
    "        plt.plot(trajectory[:, 0], trajectory[:, 1], alpha=0.2, color=\"gray\")\n",
    "\n",
    "    plt.colorbar(label=colorbar_label)\n",
    "    plt.title(title)\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Hope its clear that we are trying to predict the distance from origin\n",
    "plot_trajectory(X_test, y_test, is_regression=IS_REGRESSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "*Observe the scatterplot above. Hypothesize what underlying function would allow us to generate this data. Concretely, write down the mathematical operators (`sin`, `pow`, `exp`, etc.) that would be useful to discover the underlying function.*\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Spoiler! Click to see the answer!</summary>\n",
    "  y increases linearly with abs(X). Also, the rate of increase of y is higher for X > 0 than for X <= 0.\n",
    "</details>\n",
    "\n",
    "Some useful operators are:\n",
    "- `mul: (np.array, np.array) -> (np.array)`: Return elementwise multiplication of two arrays of same shape.\n",
    "- `@TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: DSL Generation\n",
    "- Cell 1: Predefined DSL. \n",
    "    - Exercise: Augment the DSL with the operators you wrote down earlier. This is most likely the hardest part of this tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import neurosym as ns\n",
    "from neurosym.examples import near\n",
    "\n",
    "\n",
    "def simple_dsl(L, O):\n",
    "    dslf = ns.DSLFactory(L=L, O=O, max_overall_depth=5)\n",
    "    dslf.typedef(\"fL\", \"{f, $L}\")\n",
    "\n",
    "    \"YOUR CODE HERE\"\n",
    "\n",
    "    dslf.prune_to(\"[{f, $L}] -> [{f, $O}]\")\n",
    "    return dslf.finalize()\n",
    "\n",
    "\n",
    "dsl = simple_dsl(X_test.shape[-1], y_test.shape[-1])\n",
    "print(dsl.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "*Augment the DSL above with the operators you wrote down earlier.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Program Generation\n",
    "\n",
    "We're going to use neural guided search to search for the program in the DSL that maximally fits the dataset.\n",
    "\n",
    "- Cell 1: Define a pytorch dataset from the saved data.\n",
    "- Cell 2: Define `neural_dsl`. This DSL extend the DSL we created with _neural modules_.\n",
    "- Cell 3: Define optimization functions to train module parameters.\n",
    "- Cell 4: Define a stopping condition for the search.\n",
    "- Cell 5: Define the search space and initiate the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_factory(train_seed):\n",
    "    return ns.DatasetWrapper(\n",
    "        ns.DatasetFromNpy(\n",
    "            \"../data/regression_example/train_ex_data.npy\",\n",
    "            \"../data/regression_example/train_ex_labels.npy\",\n",
    "            train_seed,\n",
    "        ),\n",
    "        ns.DatasetFromNpy(\n",
    "            \"../data/regression_example/test_ex_data.npy\",\n",
    "            \"../data/regression_example/test_ex_labels.npy\",\n",
    "            None,\n",
    "        ),\n",
    "        batch_size=200,\n",
    "    )\n",
    "\n",
    "\n",
    "datamodule = dataset_factory(42)\n",
    "input_dim, output_dim = datamodule.train.get_io_dims()\n",
    "if IS_REGRESSION:\n",
    "    output_dim = 1\n",
    "print(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ns.TypeDefiner(L=input_dim, O=output_dim)\n",
    "t.typedef(\"fL\", \"{f, $L}\")\n",
    "t.typedef(\"fO\", \"{f, $O}\")\n",
    "neural_dsl = near.NeuralDSL.from_dsl(\n",
    "    dsl=dsl,\n",
    "    modules={\n",
    "        **near.create_modules(\n",
    "            \"mlp\",\n",
    "            [t(\"($fL) -> $fL\"), t(\"($fL) -> $fO\"), t(\"($fL) -> {f, 1}\")],\n",
    "            near.mlp_factory(hidden_size=10),\n",
    "        ),\n",
    "        **near.create_modules(\n",
    "            \"rnn_seq2seq\",\n",
    "            [t(\"([$fL]) -> [$fL]\"), t(\"([$fL]) -> [$fO]\"), t(\"([$fL]) -> [{f, 1}]\")],\n",
    "            near.rnn_factory_seq2seq(hidden_size=10),\n",
    "        ),\n",
    "        **near.create_modules(\n",
    "            \"rnn_seq2class\",\n",
    "            [t(\"([$fL]) -> $fL\"), t(\"([$fL]) -> {f, 1}\")],\n",
    "            near.rnn_factory_seq2class(hidden_size=10),\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = ns.import_pytorch_lightning()\n",
    "\n",
    "\n",
    "def regression_mse_loss(\n",
    "    predictions: torch.Tensor, targets: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Not used. Retained for reference for implementing a regression loss.\n",
    "    \"\"\"\n",
    "    predictions = predictions.view(-1, predictions.shape[-1])\n",
    "    targets = targets.view(-1, targets.shape[-1])\n",
    "    return torch.nn.functional.mse_loss(predictions, targets)\n",
    "\n",
    "\n",
    "def regression_smooth_l1_loss(\n",
    "    predictions: torch.Tensor, targets: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    MSE loss is sensitive to outliers. Smooth L1 loss deals with outliers by\n",
    "    using MSE loss when L1 distance is less than beta and a diminished\n",
    "    L1 loss otherwise.\n",
    "    \"\"\"\n",
    "    \"YOUR CODE HERE\"\n",
    "\n",
    "\n",
    "trainer_cfg = near.NEARTrainerConfig(\n",
    "    lr=1e-3,\n",
    "    max_seq_len=100,\n",
    "    n_epochs=30,\n",
    "    num_labels=output_dim,\n",
    "    train_steps=len(datamodule.train),\n",
    "    loss_callback=regression_smooth_l1_loss,\n",
    "    scheduler=\"none\",\n",
    "    optimizer=torch.optim.Adam,\n",
    ")\n",
    "\n",
    "validation_cost = near.ValidationCost(\n",
    "    trainer_cfg=trainer_cfg,\n",
    "    neural_dsl=neural_dsl,\n",
    "    datamodule=datamodule,\n",
    "    enable_model_summary=False,\n",
    "    enable_progress_bar=False,\n",
    "    progress_by_epoch=True,\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5)\n",
    "    ],\n",
    "    check_val_every_n_epoch=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_depth = 7\n",
    "g = ______  # TODO: Instantiate the search graph\n",
    "\n",
    "iterator = ______  # TODO: Instantiate the bounded astar search algorithm\n",
    "best_program_nodes = []\n",
    "# Let's collect the top four programs\n",
    "while len(best_program_nodes) <= 3:\n",
    "    try:\n",
    "        node = next(iterator)\n",
    "        cost = validation_cost(node)\n",
    "        best_program_nodes.append((node, cost))\n",
    "    except StopIteration:\n",
    "        print(\"No more programs found.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Program Inspection\n",
    "\n",
    "A key benefit of program synthesis is that the output program is interpretable.\n",
    "\n",
    "- Cell 1: Render the best program.\n",
    "- Cell 2: Visualize the output space of the best program.\n",
    "    - Exercise: Does the program discovered line up with your initial hypotheses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expression(sexpr, x=0.5, y=1, level=1, dx=0.1, ax=None, text_offset=0.02):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.set_axis_off()\n",
    "\n",
    "    ax.text(\n",
    "        x,\n",
    "        y,\n",
    "        sexpr.symbol,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(facecolor=\"white\", edgecolor=\"black\", boxstyle=\"round,pad=0.5\"),\n",
    "    )\n",
    "\n",
    "    num_children = len(sexpr.children)\n",
    "    if num_children > 0:\n",
    "        child_y = y - 1 / level\n",
    "        for i, child in enumerate(sexpr.children):\n",
    "            child_x = x - (dx * (num_children - 1) / 2) + i * dx\n",
    "            ax.plot([x, child_x], [y - text_offset, child_y + text_offset], \"k-\")\n",
    "            plot_expression(\n",
    "                child,\n",
    "                x=child_x,\n",
    "                y=child_y,\n",
    "                level=level + 1,\n",
    "                dx=dx / 2,\n",
    "                ax=ax,\n",
    "                text_offset=text_offset,\n",
    "            )\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "best_program_nodes = sorted(best_program_nodes, key=lambda x: x[1])\n",
    "for i, (node, cost) in enumerate(best_program_nodes):\n",
    "    print(\n",
    "        \"({i}) Cost: {cost:.4f}, {program}\".format(\n",
    "            i=i, program=ns.render_s_expression(node.program), cost=cost\n",
    "        )\n",
    "    )\n",
    "\n",
    "best_program_node = best_program_nodes[0]\n",
    "print(ns.render_s_expression(best_program_node[0].program))\n",
    "plot_expression(best_program_node[0].program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_program_node = best_program_nodes[0]\n",
    "module = near.TorchProgramModule(dsl=neural_dsl, program=best_program_node[0].program)\n",
    "# # a satisfactory set of weights. @TODO: Remove\n",
    "# # lin.weight.data = torch.tensor([[0., 1.], [0., 0.]])\n",
    "# lin.bias.data = torch.tensor([0., 0.])\n",
    "pl_model = near.NEARTrainer(module, config=trainer_cfg)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=2000,\n",
    "    devices=\"auto\",\n",
    "    accelerator=\"cpu\",\n",
    "    enable_checkpointing=False,\n",
    "    enable_model_summary=False,\n",
    "    enable_progress_bar=False,\n",
    "    logger=False,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "trainer.fit(pl_model, datamodule.train_dataloader(), datamodule.val_dataloader())\n",
    "\n",
    "grid = np.linspace(-3, 3, 100)\n",
    "xx, yy = np.meshgrid(grid, grid)\n",
    "\n",
    "X = np.stack([xx, yy], axis=-1)\n",
    "X = X.reshape(-1, 2)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = module(X.unsqueeze(0), environment=()).squeeze(0)\n",
    "Y = Y.detach().numpy()\n",
    "Y = Y.reshape(100, 100, 1)\n",
    "y = Y\n",
    "# Using imshow to plot y as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(y, origin=\"lower\", extent=(-3, 3, -3, 3), cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Distance from origin\")\n",
    "plt.title(\n",
    "    \"Output heatmap\\n{program}\".format(\n",
    "        program=ns.render_s_expression(best_program_node[0].program)\n",
    "    )\n",
    ")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output heatmap plots program output within the range `[-3, 3]` while the data we trained on was within the range `[-1, 1]`.\n",
    "\n",
    "### Exercise\n",
    "*Does the output heatmap line up with the trajectory data we visualized in part 1? Does the program and the heatmap line up with our initial hypotheses?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
